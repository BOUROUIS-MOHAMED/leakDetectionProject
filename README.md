# Leak Detection Dashboard – Smart Water Network Demo

This project is a **mini end‑to‑end demo** of a smart water network for **leak detection and monitoring**.

It combines:

- A **Python FastAPI “Fake IoT Backend”** that simulates zones, pipes, sensors and generates realistic water network readings from CSV.
- A **.NET 8 Blazor Server dashboard** that:
  - Polls the fake IoT backend on a schedule.
  - Stores readings and network topology in a local SQLite database.
  - Uses **ML.NET** to estimate leak probability per sensor/pipe.
  - Visualizes risk and history for a university‑campus style network (Library + Dormitory).

The goal is to give you something you can **present to a professor or in a demo**: you can explain the architecture, the data flow, and the ML model, not just the UI.

---

## High‑Level Architecture

### 1. Data generation & Fake IoT backend (Python / FastAPI)

**Inputs / components**

- `sensor_data.csv` – synthetic time‑series of water network events generated by `generate_sensor_data.py` (pressure, flow, leak labels, etc.).
- `fake_iot_backend.py` – FastAPI service that:
  - Loads the CSV into memory as `SensorRecord` objects grouped by sensor.
  - Exposes a REST API endpoint:

    ```http
    GET /api/v1/readings?window_minutes=5&data_points_per_sensor=5
    ```

  - Returns JSON containing:
    - `zones`: static topology of zones (Library, Dormitory, …).
    - `pipes`: static topology of pipes within each zone.
    - `sensors`: metadata + **time‑window readings** + statistics (min/avg/max pressure, flow, anomaly score, etc.).
    - `readingMetadata` and `systemHealth` (timestamps, sampling interval, latency, quality score…).

**Sliding window behaviour**

- The backend internally keeps track of a **moving index** over the CSV.
- Every time the .NET app calls `/api/v1/readings`, the backend shifts the window forward and returns a different “chunk” of data per sensor.
- From the dashboard point of view this looks like **live data** streaming from physical IoT sensors.

---

### 2. LeakDetectionDashboard (C# / Blazor Server / ML.NET)

**Main building blocks**

- **EF Core / SQLite** (`AppDbContext`):
  - `Zones`, `Pipes`, `Sensors` – network topology.
  - `SensorSnapshots` – normalized time‑series used by the dashboard and ML.
  - `Setting` – single row (Id = 1) for app configuration (e.g. polling interval).
  - `LogEntry` – app‑level logs persisted in DB.

- **Services**
  - `FakeIotClient`
    - Typed `HttpClient` that talks to the FastAPI backend.
    - Calls `/api/v1/readings` with the desired window size and points per sensor.
    - Deserializes the JSON into strongly‑typed DTOs (`IoTReadingResponse`, `SensorDto`, `TimeSeriesReadingDto`, …).
  - `SensorPollingService` (background hosted service)
    - Runs in the background on the Blazor server.
    - Every **N minutes** (value stored in `Setting.PollIntervalMinutes`) it:
      1. Calls `FakeIotClient.GetReadingsAsync(intervalMinutes)`.
      2. Passes the payload to `LeakDetectionService.ProcessReadingAsync(...)`.
  - `LeakDetectionService`
    - Upserts zones, pipes, sensors into the database (so you can see them in the UI).
    - Builds a `SensorSnapshot` for each sensor:
      - Uses last pressure/flow sample from the time window.
      - Computes derived features (pressure drop rate, water usage difference vs previous sensor, time‑of‑day features, occupancy, break type, etc.).
    - Converts each snapshot into a **feature vector** (`LeakTrainingRecord`) and calls the ML model via `LeakDetectionModelService`.
    - Saves predictions into `SensorSnapshots` (`LeakProbability`, flags like `IsLeakPredicted`, etc.).
    - Keeps only the last **24 hours** of snapshots (older ones are pruned).
    - `GetCurrentPipeRiskAsync()` aggregates snapshots by pipe and returns the **latest leak probability per pipe** (used by the Architecture view).
  - `LeakDetectionModelService`
    - ML.NET service responsible for training and loading the leak detection model.
    - Loads training data from `Data/training/leak_training_data.csv`.
    - Builds a **binary classification pipeline** (FastTree) with features:
      - Physical: `PressureCurrent`, `PressurePreviousSensor`, `FlowRate`, `WaterUsageDiff`, `PressureDropRate`.
      - Time: `Hour`, `Minute`, `DayOfWeek`.
      - Schedule / behaviour: `IsWorkingHours`, `IsBreakTime`, `ExpectedUsageMultiplier`, `MinutesSinceBreakStart`, `OccupancyLevel`.
      - Context: `PressureCurrentVsBaseline`, `FlowRateVsBaseline`.
      - Categorical: `BreakType`, `LeakType` (featurized as text).
    - Label: `IsLeak` (0/1). Output: leak probability.
    - Saves trained model to `Data/models/leak_detection_model.zip` and logs metrics (Accuracy, AUC, F1).
  - `SettingsService`
    - Simple CRUD layer for the `Setting` row.
    - Reads and updates `PollIntervalMinutes` so the background service is configurable from the UI.
  - `LoggingService`
    - Central logging point (Info / Warning / Error) that writes to both console and `LogEntries` table.

- **Blazor UI (Razor Components)**

  - `MainLayout.razor`
    - Left sidebar navigation + top app bar (“Leak Detection Dashboard”).

  - Pages:
    - `Architecture.razor`
      - Shows a **grid of zones** → inside each zone, you see **pipes** → inside each pipe, you see **sensors**.
      - Uses `LeakDetectionService.GetCurrentPipeRiskAsync()` to display a **colored risk badge** per pipe.
      - `RiskBadge` component maps probability to label + CSS class:
        - 0–20% → “Normal” (green).
        - 20–40% → “High usage”.
        - 40–60% → “Risk of leak”.
        - 60–80% → “High possibility”.
        - 80–100% → “Leak” (red gradient).
    - `History.razor`
      - Loads the last **5 hours** of `SensorSnapshots` (max 500 rows).
      - Lets you filter by Zone / Pipe / Sensor via `<select>` dropdowns.
      - Displays a table with timestamp, topology, pressure, flow, pressure drop rate, and leak probability (again using `RiskBadge`).
    - `Settings.razor`
      - Card layout describing the polling behaviour.
      - Number input: “Poll every X minutes” (1–60).
      - “Save changes” button → updates `Setting.PollIntervalMinutes` via `SettingsService`.
      - A status pill shows whether the save succeeded.
    - `Logs.razor`
      - Simple table of recent `LogEntry` rows: time, level, message.
    - `Index.razor`
      - Immediately redirects `/` → `/architecture` for convenience.

---

## Technologies

- **Backend dashboard**
  - .NET 8
  - Blazor Server (interactive server render mode)
  - Entity Framework Core (SQLite)
  - ML.NET (FastTree binary classifier)

- **Fake IoT backend**
  - Python 3
  - FastAPI
  - Uvicorn
  - CSV‑based data generation (synthetic dataset).

---

## Folder Structure (simplified)

```text
LeakDetectionDashboard/
├─ Data/
│  ├─ AppDbContext.cs
│  ├─ DesignTimeDbContextFactory.cs
│  └─ Migrations/            # EF Core migrations
├─ Models/
│  ├─ Zone.cs
│  ├─ Pipe.cs
│  ├─ Sensor.cs
│  ├─ SensorSnapshot.cs
│  ├─ Setting.cs
│  ├─ LogEntry.cs
│  ├─ Api/                   # DTOs used for JSON from FastAPI
│  └─ ML/                    # Training record + prediction types
├─ Services/
│  ├─ FakeIotClient.cs
│  ├─ SensorPollingService.cs
│  ├─ LeakDetectionService.cs
│  ├─ LeakDetectionModelService.cs
│  ├─ SettingsService.cs
│  └─ LoggingService.cs
├─ Components/
│  ├─ App.razor              # Root Blazor router
│  └─ Layout/
│     └─ MainLayout.razor
├─ Pages/
│  ├─ Architecture.razor
│  ├─ History.razor
│  ├─ Settings.razor
│  ├─ Logs.razor
│  └─ Index.razor
├─ wwwroot/
│  ├─ app.css
│  ├─ css/
│  │  └─ site.css
│  └─ LeakDetectionDashboard.styles.css
├─ Program.cs
└─ appsettings.json

fake-iot-backend/
├─ fake_iot_backend.py
├─ generate_sensor_data.py
└─ sensor_data.csv
```

---

## Getting Started

### 1. Prerequisites

- **.NET 8 SDK** installed.
- **Python 3.10+** installed.
- `git`, `pip` etc.

### 2. Setup the fake IoT backend (Python)

From the `fake-iot-backend` folder:

```bash
cd fake-iot-backend

# (Optional but recommended) create virtual env
python -m venv .venv
# Windows
.\.venv\Scriptsctivate
# Linux / macOS
# source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

Generate or regenerate sensor data:

```bash
python generate_sensor_data.py
```

Run the FastAPI server:

```bash
python fake_iot_backend.py
# or
# uvicorn fake_iot_backend:app --reload --host 0.0.0.0 --port 8000
```

The backend will listen on `http://localhost:8000` and serve:

- `/health`
- `/api/v1/readings`

If you open `/health` in a browser you should see a small JSON status payload.

### 3. Configure the .NET dashboard

In `LeakDetectionDashboard/appsettings.json` make sure you have something like:

```json
{
  "ConnectionStrings": {
    "DefaultConnection": "Data Source=leak_detection.db"
  },
  "FakeIot": {
    "BaseUrl": "http://localhost:8000",
    "ReadingsEndpoint": "/api/v1/readings"
  },
  "ML": {
    "TrainingDataPath": "Data/training/leak_training_data.csv",
    "ModelPath": "Data/models/leak_detection_model.zip"
  },
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  }
}
```

- `BaseUrl` and `ReadingsEndpoint` must match your Python server.
- `TrainingDataPath` and `ModelPath` point to the ML training CSV and model file used by ML.NET.

### 4. Initialize the database

From the `LeakDetectionDashboard` folder:

```bash
cd LeakDetectionDashboard

# If EF tools are not installed yet:
dotnet tool install --global dotnet-ef

# Apply migrations
dotnet ef database update
```

This will create the SQLite DB file (for example `leak_detection.db`) with all tables and seed the initial `Setting` row with `PollIntervalMinutes = 5`.

### 5. Run the dashboard

Still in the `LeakDetectionDashboard` folder:

```bash
dotnet run
```

By default Kestrel will listen on something like:

- `http://localhost:5000` (HTTP)
- Optionally `https://localhost:5001` if HTTPS is enabled.

Open the URL in your browser. The app will redirect `/` to `/architecture`.

Make sure the **Python fake backend is already running**; otherwise the background polling service will log warnings (and you’ll see empty readings until the backend is available).

---

## How the Polling Interval Works

- The current poll interval (in minutes) is stored in the database table `Settings` (row with `Id = 1`).  
- `SettingsService.GetPollIntervalMinutesAsync()` reads it.
- The `SensorPollingService` uses this value to decide how often to:

  1. Call `FakeIotClient.GetReadingsAsync(intervalMinutes)`.
  2. Forward the JSON payload to `LeakDetectionService.ProcessReadingAsync(...)`.

- On the **Settings page** you can:
  - See the current value.
  - Change it via the number input (“Poll every X minutes”) and click **“Save changes”**.
  - A status pill shows whether the save succeeded.

The background service will automatically use the new value on its **next loop** – you do **not** need to restart the application.

---

## Explaining the ML Model (for your professor)

- We train a **binary classifier** with ML.NET’s `FastTree` algorithm:
  - Target/label: `IsLeak` (1 if the row represents a leak, 0 otherwise).
  - Input features mix **physical**, **temporal**, and **behavioural** variables.
- During training:
  - Data is loaded from `leak_training_data.csv` into `LeakTrainingRecord` objects.
  - We create text features from `BreakType` and `LeakType` (categorical) and concatenate them with all numerical features into a `Features` vector.
  - We split the dataset into **train (80%) / test (20%)** and compute metrics (Accuracy, AUC, F1) to validate the model.
  - The trained model is saved to `Data/models/leak_detection_model.zip`.

- At runtime:
  - Each new `SensorSnapshot` is mapped back to `LeakTrainingRecord` (without the `IsLeak` label).
  - `LeakDetectionModelService.PredictLeakProbability(...)` returns a probability in `[0,1]`.
  - This probability is stored in the snapshot and visualised in the UI as a percentage + risk badge.

This gives you a clean story to present:
> “We simulate data → store it → engineer features → train an ML model → run that model live on incoming readings → visualize risk on the dashboard.”

---

## Extending the Project

Some ideas for future improvements:

- Add more **zones and pipes** to better simulate a full campus or city block.
- Add a **charting page** (per sensor) with line graphs for pressure/flow vs time.
- Expose an **API** from the .NET side so other systems can query current leak risk.
- Swap the fake backend with a real IoT gateway that connects to real sensors.
- Experiment with more advanced models (e.g. time‑series models or anomaly detection) and compare metrics.

---

## License

This project is intended as a **teaching/demo project**.  
You can add whatever license you prefer (for example, MIT) in a separate `LICENSE` file.
